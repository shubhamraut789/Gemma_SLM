# Gemma_SLM

• Trained a SentencePiece tokeniser, ensuring effective token coverage for all three languages.
• Built a multilingual autoregressive language model (Gemma dense, 106M parameters) trained on 3B+ tokens across English(L1), Marathi(L2), and Konkani(L3).
• Pretrained and fine-tuned the model for sentence formation and L1→L3 translation tasks, enhancing multilingual reasoning and generation in low-resource settings.
